# LangChain ê´€ë ¨ ì½”ë“œ
import os
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def get_llm(temperature=0.3):
    try:
        api_key = st.secrets["OPENAI_API_KEY"]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        st.error("ğŸš¨ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        st.info("Streamlit Cloud: Settings > Secretsì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()
    
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=temperature,
        openai_api_key=api_key
    )

def run_chain(system_prompt, user_input, temperature=0.3):
    # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "{text}")
    ])
    
    # 2. ì²´ì¸ êµ¬ì„±
    chain = prompt | get_llm(temperature) | StrOutputParser()
    
    # 3. ì‹¤í–‰
    result = chain.invoke({"text": user_input})
    return result

def run_chain_with_memory(system_prompt, user_input, memory, temperature=0.3):
    """Memoryë¥¼ ì‚¬ìš©í•˜ëŠ” ì²´ì¸ (session_state ë°©ì‹)"""
    from utils.memory_handler import format_history_for_prompt
    
    # 1. ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    history_text = format_history_for_prompt(memory)
    
    # 2. í”„ë¡¬í”„íŠ¸ì— íˆìŠ¤í† ë¦¬ í¬í•¨
    full_prompt = system_prompt + history_text
    
    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", full_prompt),
        ("user", "{text}")
    ])
    
    # 4. ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
    chain = prompt | get_llm(temperature) | StrOutputParser()
    result = chain.invoke({"text": user_input})
    
    return result