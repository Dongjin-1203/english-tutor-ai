# LangChain ê´€ë ¨ ì½”ë“œ
import os
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
def get_llm(temperature=0.3):
    try:
        api_key = st.secrets["OPENAI_API_KEY"]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        st.error("ğŸš¨ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        st.info("Streamlit Cloud: Settings > Secretsì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()
    
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=temperature,
        openai_api_key=api_key
    )

def run_chain(system_prompt, user_input, temperature=0.3):
    # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "{text}")
    ])
    
    # 2. ì²´ì¸ êµ¬ì„±
    chain = prompt | get_llm(temperature) | StrOutputParser()
    
    # 3. ì‹¤í–‰
    result = chain.invoke({"text": user_input})
    return result

def run_chain_with_memory(system_prompt, user_input, memory, temperature=0.3):
    # 1. ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    chat_history = memory.load_memory_variables({})
    
    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (MessagesPlaceholder ì‚¬ìš©!)
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),  # âœ… ì´ë ‡ê²Œ ìˆ˜ì •!
        ("user", "{input}")
    ])
    
    # 3. ì²´ì¸ êµ¬ì„±
    chain = prompt | get_llm(temperature) | StrOutputParser()
    
    # 4. ì‹¤í–‰ (chat_history ì „ë‹¬)
    result = chain.invoke({
        "input": user_input,
        "chat_history": chat_history.get("history", [])
    })
    
    # 5. ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
    memory.save_context(
        {"input": user_input},
        {"output": result}
    )
    
    return result